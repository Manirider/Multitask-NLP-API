# ==========================================
# MLflow Configuration
# ==========================================
# URL for the MLflow tracking server. 
# Usage: http://localhost:5000 when running locally with Docker.
MLFLOW_TRACKING_URI=http://mlflow:5000

# Name of the experiment in MLflow dashboard.
MLFLOW_EXPERIMENT_NAME=multitask_nlp_experiment

# ==========================================
# Training Configuration
# ==========================================
# Base Transformer model to use. 
# Options: distilbert-base-uncased, bert-base-uncased
MODEL_NAME=distilbert-base-uncased

# Maximum sequence length for tokenization. 
# Larger values require more memory. Default: 128
MAX_SEQ_LENGTH=128

# Batch size per task.
# Recommendations: 16 (for 8GB RAM), 32 (for 16GB RAM)
BATCH_SIZE=16

# Number of training epochs.
# 3 epochs is typically sufficient for fine-tuning DistilBERT on these datasets.
EPOCHS=3

# Learning rate for the AdamW optimizer.
LEARNING_RATE=2e-5

# ==========================================
# Application Settings
# ==========================================
# API Metadata
APP_NAME=Multi-Task NLP API
APP_VERSION=1.0.0
DEBUG=false
